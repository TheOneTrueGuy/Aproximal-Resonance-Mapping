# ARM Critical Validation Plan â€” October 1, 2025

## Priority Order for Addressing Criticisms

This document outlines exercises to rigorously validate ARM's mathematical foundations,
practical utility, and claims. Based on criticisms in Criticisms.txt and 
math_rigor_criticisms-observations.md.

================================================================================
SECTION 1: MATHEMATICAL RIGOR & TERMINOLOGY
================================================================================

### Exercise 1.1: Audit "Resonance" Terminology
**Goal**: Verify whether "resonance" is justified or just metaphorical
**Tasks**:
  [ ] Read through all code/docs and list every use of "resonance" terminology
  [ ] For each usage, identify the actual mathematical operation (SVD, eigendecomp, etc.)
  [ ] Document what physical/mathematical property would justify calling it "resonance"
  [ ] Decide: Keep with formal definition, or rename to "spectral analysis"?
**Deliverable**: resonance_audit.md with findings and recommendation
**Time**: 2 hours

### Exercise 1.2: Formalize Spectral Analysis
**Goal**: Replace vague "resonance" with precise mathematical terminology
**Tasks**:
  [ ] Review resonance_analyzer.py - what is actually being computed?
  [ ] Verify: Are we computing SVD of centered activation matrices? Yes/No
  [ ] Verify: Are singular values being interpreted as spectral magnitudes? Yes/No
  [ ] Check if entropy/participation ratio have precedent in manifold learning literature
  [ ] Rename module to either:
      Option A: spectral_analyzer.py (if dropping "resonance")
      Option B: Keep name but add formal mathematical definition in docstring
**Deliverable**: Updated module with rigorous mathematical definitions
**Time**: 3 hours

### Exercise 1.3: Validate "Oscillatory Response" Claims
**Goal**: Test if activations actually exhibit oscillatory behavior
**Tasks**:
  [ ] Generate probe responses for 10 diverse prompts
  [ ] Plot activation values along each probe direction
  [ ] Look for: periodic patterns, damping, phase relationships
  [ ] Statistical test: Compare to random walk (should be different if oscillatory)
  [ ] Document: Do we see actual oscillations or just decay/monotonic change?
**Deliverable**: oscillation_validation.ipynb with plots and statistical tests
**Time**: 4 hours

================================================================================
SECTION 2: TOPOLOGY COMPONENT VALIDATION
================================================================================

### Exercise 2.1: Ablation Study - Topology vs Spectral Only
**Goal**: Determine if topology features actually contribute to steering performance
**Tasks**:
  [ ] Create 3 versions of descriptor computation:
      Version A: Spectral only (s_norm, entropy, PR)
      Version B: Topology only (persistence features)
      Version C: Combined (current implementation)
  [ ] Run JSON adherence eval with each version
  [ ] Run style transfer eval with each version
  [ ] Compare: Does combined beat spectral-only? If not, topology may be vestigial
**Deliverable**: ablation_results.csv with performance comparison
**Time**: 4 hours

### Exercise 2.2: Topology Interpretability
**Goal**: Understand what persistent homology features represent behaviorally
**Tasks**:
  [ ] Run ARM on 20 seeds spanning different content types
  [ ] For each seed, record: H0 persistence, H1 persistence, behavioral category
  [ ] Look for patterns: Do seeds with similar behavior have similar topology?
  [ ] Test correlation: persistence metrics vs. clustering quality
  [ ] Visualize: persistence diagrams colored by behavioral category
**Deliverable**: topology_interpretation.ipynb with correlation analysis
**Time**: 5 hours

### Exercise 2.3: Synthetic Manifold Validation
**Goal**: Test topology extraction on manifolds with known ground truth
**Tasks**:
  [ ] Create synthetic datasets with known topology:
      - Circle manifold (H1 = 1 loop)
      - Sphere (H2 = 1 void)
      - Torus (H1 = 2 loops)
  [ ] Generate "fake" activations embedded in high-dim space
  [ ] Run ARM topology mapper on these
  [ ] Verify: Does persistent homology recover the known topology?
**Deliverable**: synthetic_topology_test.py with ground truth comparisons
**Time**: 6 hours

================================================================================
SECTION 3: BASELINE COMPARISONS
================================================================================

### Exercise 3.1: RepE Control Vector Baseline
**Goal**: Compare ARM steering against standard Representation Engineering
**Tasks**:
  [ ] Implement simple RepE: mean(positive) - mean(negative) at layer L
  [ ] Run JSON task with RepE steering at strengths [0.0, 0.5, 1.0, 2.0]
  [ ] Run same task with ARM manifold-signature steering
  [ ] Compare: JSON adherence, perplexity, fluency scores
  [ ] Document: When does ARM outperform? When is RepE sufficient?
**Deliverable**: repe_comparison.csv and analysis in baseline_comparisons.md
**Time**: 4 hours

### Exercise 3.2: Few-Shot Prompting Baseline
**Goal**: Verify ARM adds value beyond just better prompting
**Tasks**:
  [ ] Establish few-shot baseline for JSON task (0-shot, 1-shot, 3-shot, 5-shot)
  [ ] Measure JSON adherence for each few-shot condition
  [ ] Compare: ARM with 2-shot context vs. pure 5-shot prompting
  [ ] Document: Does ARM beat strong prompting? Or is it redundant?
**Deliverable**: few_shot_baseline.csv with comparison analysis
**Time**: 3 hours

### Exercise 3.3: Fine-Tuning Baseline (Optional - Resource Intensive)
**Goal**: Upper bound comparison - how does ARM compare to task-specific fine-tuning?
**Tasks**:
  [ ] Fine-tune distilgpt2 on 100 JSON examples (5 epochs)
  [ ] Test JSON adherence of fine-tuned model
  [ ] Compare to ARM steering (which uses no gradient updates)
  [ ] Document: ARM as "zero-shot fine-tuning" vs. actual fine-tuning
**Deliverable**: finetuning_comparison.md (only if time/resources permit)
**Time**: 8 hours (skip if compute-limited)

================================================================================
SECTION 4: MULTI-TASK EVALUATION
================================================================================

### Exercise 4.1: Expand Task Coverage
**Goal**: Move beyond single JSON task to diverse evaluation
**Tasks**:
  [ ] Implement 4 additional evaluation tasks:
      Task 1: Style transfer (current - already working)
      Task 2: Sentiment steering (positive vs negative)
      Task 3: Formality control (casual vs formal)
      Task 4: Factuality (correct vs incorrect facts)
  [ ] Run ARM manifold-signature on each task
  [ ] Measure task-specific metrics for each
  [ ] Report: Success rate across diverse behavioral controls
**Deliverable**: multi_task_eval.csv with 5 tasks Ã— multiple metrics
**Time**: 8 hours

### Exercise 4.2: Cross-Model Validation
**Goal**: Test if ARM generalizes across model architectures
**Tasks**:
  [ ] Run JSON task on 3 models: distilgpt2, gpt2, gpt2-medium
  [ ] Run sentiment task on same 3 models
  [ ] Compare: Does ARM work consistently? Model-specific tuning needed?
  [ ] Check: Do manifold structures differ significantly across models?
**Deliverable**: cross_model_results.csv and analysis
**Time**: 6 hours

### Exercise 4.3: Statistical Significance Testing
**Goal**: Ensure results aren't just random fluctuations
**Tasks**:
  [ ] For JSON task, run ARM with 10 different random seeds
  [ ] Compute: mean, std, 95% confidence interval for JSON adherence
  [ ] Run t-test: ARM vs baseline (Î± = 0.05)
  [ ] Report: Is improvement statistically significant?
  [ ] Repeat for 2-3 other tasks
**Deliverable**: significance_tests.csv with p-values and effect sizes
**Time**: 4 hours

================================================================================
SECTION 5: MANIFOLD CONSTRUCTION ROBUSTNESS
================================================================================

### Exercise 5.1: Seed Count Sensitivity Analysis
**Goal**: Determine minimum viable seeds for meaningful topology
**Tasks**:
  [ ] Run ARM with varying seed counts: [2, 5, 10, 20, 50, 100]
  [ ] For each count, measure:
      - Silhouette score (clustering quality)
      - Steering effectiveness (JSON adherence)
      - Computation time
  [ ] Find: Minimum seeds for stable performance
  [ ] Plot: Performance vs. seed count (diminishing returns curve)
**Deliverable**: seed_sensitivity.png and recommendations
**Time**: 5 hours

### Exercise 5.2: Probe Configuration Optimization
**Goal**: Validate default probe settings (probes_per_seed, steps_per_probe, eps)
**Tasks**:
  [ ] Grid search over configurations:
      probes_per_seed: [4, 8, 16, 32]
      steps_per_probe: [3, 5, 9, 15]
      eps: [0.01, 0.03, 0.05, 0.1]
  [ ] For each, measure: steering effectiveness, computation time
  [ ] Find: Pareto frontier (performance vs. cost)
  [ ] Validate: Are current defaults optimal or just arbitrary?
**Deliverable**: probe_optimization.csv and recommended configs
**Time**: 8 hours (can subsample grid if needed)

### Exercise 5.3: Manifold Stability Test
**Goal**: Check if manifold structure is consistent across runs
**Tasks**:
  [ ] Build manifold from same 10 seeds with 5 different random_seed values
  [ ] Compute descriptor correlation between runs (should be high)
  [ ] Measure cluster assignment stability (Adjusted Rand Index)
  [ ] Test: If random_seed changes, does steering effectiveness change?
  [ ] Document: Is ARM deterministic given fixed random_seed?
**Deliverable**: stability_analysis.md with reproducibility assessment
**Time**: 4 hours

================================================================================
SECTION 6: COMPUTATIONAL EFFICIENCY
================================================================================

### Exercise 6.1: Profile Performance Bottlenecks
**Goal**: Identify where computation time is spent
**Tasks**:
  [ ] Use cProfile or line_profiler on map_latent_manifold()
  [ ] Break down time by component:
      - Model forward passes
      - SVD computation
      - Persistent homology (ripser)
      - Graph construction
      - Clustering
  [ ] Identify: Which component is the bottleneck?
**Deliverable**: profiling_results.txt with time breakdown
**Time**: 2 hours

### Exercise 6.2: Scaling Analysis
**Goal**: Quantify computational cost vs. manifold size
**Tasks**:
  [ ] Measure wall-clock time for different (seeds Ã— probes Ã— steps):
      Small: 5 Ã— 4 Ã— 3 = 60 forward passes
      Medium: 20 Ã— 8 Ã— 5 = 800 forward passes
      Large: 50 Ã— 16 Ã— 9 = 7,200 forward passes
  [ ] Measure peak memory usage for each
  [ ] Plot: Time/Memory vs. total forward passes (linear? quadratic?)
  [ ] Extrapolate: Cost for production-scale analysis
**Deliverable**: scaling_analysis.png with cost projections
**Time**: 3 hours

### Exercise 6.3: Optimization Opportunities
**Goal**: Identify ways to reduce computational cost
**Tasks**:
  [ ] Test batch processing: Forward multiple probes simultaneously
  [ ] Test caching: Reuse base activations across similar prompts
  [ ] Test approximations: TruncatedSVD vs. full SVD
  [ ] Measure: Speedup from each optimization
  [ ] Document: Speed vs. quality tradeoffs
**Deliverable**: optimization_recommendations.md with speedup measurements
**Time**: 6 hours

================================================================================
SECTION 7: CLAIMS VALIDATION
================================================================================

### Exercise 7.1: "Emergent Recognition" Claim
**Goal**: Test if boundaries emerge without manual labels
**Tasks**:
  [ ] Build manifold from mixed prompts (no labeling)
  [ ] Use clustering to auto-detect groups
  [ ] Compare auto-detected groups to ground truth categories
  [ ] Measure: Cluster purity, Adjusted Rand Index
  [ ] Verify: Does ARM find meaningful boundaries or just arbitrary clusters?
**Deliverable**: emergent_recognition_test.ipynb with purity scores
**Time**: 4 hours

### Exercise 7.2: "Richer than Linear Control Vectors" Claim
**Goal**: Quantify how much richer ARM manifolds are vs. single vectors
**Tasks**:
  [ ] Compare representational capacity:
      Linear: 1 direction in hidden_dim space
      ARM: Full descriptor dimensionality
  [ ] Test: Can linear vector replicate ARM steering? (project ARM to 1D)
  [ ] Measure: Behavioral diversity captured by ARM vs. linear
  [ ] Quantify: Effective dimensionality of ARM control surface
**Deliverable**: richness_comparison.md with dimensionality analysis
**Time**: 5 hours

### Exercise 7.3: "Topological Generalization" Claim
**Goal**: Verify ARM captures structures beyond linear methods
**Tasks**:
  [ ] Create scenario where topology should matter (e.g., circular relationships)
  [ ] Test: Does ARM detect topology? Does linear method fail?
  [ ] Example: "morningâ†’afternoonâ†’eveningâ†’nightâ†’morning" (circular)
  [ ] Verify: H1 persistence should detect the loop
  [ ] Document: Concrete examples where topology provides value
**Deliverable**: topology_value_demonstration.ipynb with case studies
**Time**: 6 hours

================================================================================
SUMMARY & PRIORITIZATION
================================================================================

IMMEDIATE PRIORITY (Do First):
  [X] Exercise 1.1 - Audit "Resonance" Terminology - COMPLETE âœ“
      Result: No empirical oscillations. "Resonance" metaphorical only.
      Deliverables: resonance_audit.md, test_transformer_oscillations.py
  [X] Exercise 2.1 - Ablation Study (Topology vs Spectral) - COMPLETE âœ“
      Result: Topology redundant, actually hurts when combined. Use spectral only.
      Deliverables: exercise_2_1_ablation_fixed.py, ablation results
  [ ] Exercise 3.1 - RepE Baseline Comparison - NEXT UP
  [ ] Exercise 3.2 - Few-Shot Baseline
  [ ] Exercise 4.3 - Statistical Significance Testing

HIGH PRIORITY (Critical Validation):
  [6] Exercise 4.1 - Multi-Task Evaluation
  [7] Exercise 5.1 - Seed Count Sensitivity
  [8] Exercise 2.2 - Topology Interpretability
  [9] Exercise 1.2 - Formalize Spectral Analysis

MEDIUM PRIORITY (Robustness):
  [10] Exercise 4.2 - Cross-Model Validation
  [11] Exercise 5.3 - Manifold Stability
  [12] Exercise 6.1 - Profile Performance
  [13] Exercise 7.1 - "Emergent Recognition" Validation

LOWER PRIORITY (Nice to Have):
  [14] Exercise 1.3 - Oscillatory Response Validation
  [15] Exercise 2.3 - Synthetic Manifold Validation
  [16] Exercise 5.2 - Probe Configuration Optimization
  [17] Exercise 6.2 - Scaling Analysis
  [18] Exercise 7.2 - Richness Comparison
  [19] Exercise 7.3 - Topological Generalization Demo

OPTIONAL (Resource Intensive):
  [20] Exercise 3.3 - Fine-Tuning Baseline
  [21] Exercise 6.3 - Optimization Opportunities

================================================================================
EXECUTION STRATEGY
================================================================================

Week 1: Foundation & Quick Wins
  - Days 1-2: Exercises 1.1, 1.2 (terminology cleanup)
  - Days 3-4: Exercise 2.1 (ablation study - critical!)
  - Day 5: Exercise 3.1 (RepE baseline)

Week 2: Comprehensive Validation
  - Days 1-2: Exercise 4.1 (multi-task eval)
  - Days 3-4: Exercise 4.3 (statistical testing)
  - Day 5: Exercise 5.1 (seed sensitivity)

Week 3: Deep Dive & Documentation
  - Days 1-2: Exercise 2.2 (topology interpretation)
  - Days 3-4: Exercise 4.2 (cross-model)
  - Day 5: Compile findings into validation_report.md

================================================================================
SUCCESS CRITERIA
================================================================================

After completing exercises, we should be able to answer:

1. Is "resonance" terminology justified? â†’ Exercise 1.1, 1.2, 1.3
2. Does topology contribute or is it vestigial? â†’ Exercise 2.1, 2.2, 2.3
3. Is ARM better than simpler baselines? â†’ Exercise 3.1, 3.2
4. Does ARM generalize beyond JSON task? â†’ Exercise 4.1, 4.2
5. Are results statistically significant? â†’ Exercise 4.3
6. How many seeds needed for stable manifolds? â†’ Exercise 5.1, 5.3
7. What are computational costs? â†’ Exercise 6.1, 6.2
8. Do key claims hold up to scrutiny? â†’ Exercise 7.1, 7.2, 7.3

If answers are satisfactory, ARM is on solid ground.
If answers are negative, we pivot to simpler/better-justified approaches.

================================================================================
NOTES & REMINDERS
================================================================================

- Be ruthlessly honest with results - negative findings are valuable
- Document everything - failed experiments teach us too
- If topology proves vestigial, don't be afraid to simplify
- If RepE performs equally well, ARM needs better justification
- Keep compute costs in mind - practical utility matters

Good luck! Let's validate ARM rigorously. ðŸ”¬

