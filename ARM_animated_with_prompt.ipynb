{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft0OeiKIYcNf",
        "outputId": "15064087-1174-4d17-873b-d3e78bd5751d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.35.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers transformers torch\n",
        "!pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvejWqh6xGxI",
        "outputId": "2c9508a1-27e5-4f09-ac28-1ee17597dbf9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (0.35.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from diffusers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.35.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from diffusers) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from diffusers) (0.6.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from diffusers) (11.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.34.0->diffusers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->diffusers) (2025.8.3)\n",
            "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "Successfully installed transformers-4.56.2\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xformers) (2.0.2)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from xformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->xformers) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0->xformers) (3.0.2)\n",
            "Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.32.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.cuda.empty_cache()\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL_Hw6P15OLO",
        "outputId": "b1894429-dd59-4a1f-c6e4-bce4b18f0074"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.35.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "#!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn imageio -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. Load SSD-1B pipeline ---\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"segmind/SSD-1B\",\n",
        "    torch_dtype=torch.float16\n",
        ").to(device)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "prompt_embeds = pipe.encode_prompt(prompt)\n",
        "print(f\"prompt_embeds shape: {prompt_embeds.shape}\")\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1, 6, i + 1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")\n"
      ],
      "metadata": {
        "id": "TUMXpmVy63Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "# Enable memory optimizations\n",
        "pipe.enable_model_cpu_offload()  # Primary memory fix\n",
        "pipe.enable_vae_tiling()\n",
        "pipe.enable_xformers_memory_efficient_attention()  # Uncomment if xformers is installed\n",
        "vae = pipe.vae\n",
        "print(\"U-Net config:\", pipe.unet.config)\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "prompt_embeds, _, pooled_embeds, _ = pipe.encode_prompt(\n",
        "    prompt=prompt,\n",
        "    device=device,\n",
        "    num_images_per_prompt=1,\n",
        "    do_classifier_free_guidance=False\n",
        ")\n",
        "print(f\"prompt_embeds shape: {prompt_embeds.shape}\")\n",
        "print(f\"pooled_embeds shape: {pooled_embeds.shape}\")\n",
        "time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], device=device, dtype=torch.float16)\n",
        "print(f\"time_ids shape: {time_ids.shape}\")\n",
        "\n",
        "# Concatenate pooled_embeds and time_ids for added_cond_kwargs\n",
        "added_cond_embeds = torch.cat([pooled_embeds, time_ids], dim=-1)\n",
        "print(f\"added_cond_embeds shape: {added_cond_embeds.shape}\")\n",
        "\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64), # Reduced resolution\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    print(f\"Step {i}, latent_model_input shape: {latent_model_input.shape}\")\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": added_cond_embeds, # Use the concatenated embeddings\n",
        "            \"time_ids\": time_ids # Still need time_ids for other parts of the UNet\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1, 6, i + 1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")"
      ],
      "metadata": {
        "id": "sW8oI_715ta-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKeGXYgx5tY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "# Set PyTorch memory configuration\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "# Enable memory optimizations\n",
        "pipe.enable_model_cpu_offload()  # Primary memory fix\n",
        "pipe.enable_vae_tiling()\n",
        "pipe.enable_xformers_memory_efficient_attention()  # Uncomment if xformers is installed\n",
        "vae = pipe.vae\n",
        "print(\"U-Net config:\", pipe.unet.config)\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "prompt_embeds, _, pooled_embeds, _ = pipe.encode_prompt(\n",
        "    prompt=prompt,\n",
        "    device=device,\n",
        "    num_images_per_prompt=1,\n",
        "    do_classifier_free_guidance=False\n",
        ")\n",
        "print(f\"prompt_embeds shape: {prompt_embeds.shape}\")\n",
        "print(f\"pooled_embeds shape: {pooled_embeds.shape}\")\n",
        "time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], device=device, dtype=torch.float16)\n",
        "print(f\"time_ids shape: {time_ids.shape}\")\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64), # Reduced resolution\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    print(f\"Step {i}, latent_model_input shape: {latent_model_input.shape}\")\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": pooled_embeds,\n",
        "            \"time_ids\": time_ids\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1, 6, i + 1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")"
      ],
      "metadata": {
        "id": "-ShcHKOu3Vx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "torch.cuda.empty_cache()\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "print(\"U-Net config:\", pipe.unet.config)\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Use pipe.encode_prompt to handle both text encoders\n",
        "prompt_embeds, _, pooled_embeds, _ = pipe.encode_prompt(\n",
        "    prompt=prompt,\n",
        "    device=device,\n",
        "    num_images_per_prompt=1,\n",
        "    do_classifier_free_guidance=False\n",
        ")\n",
        "\n",
        "# Ensure shapes are correct\n",
        "print(f\"prompt_embeds shape: {prompt_embeds.shape}\")  # Expected: (1, 77, 2048)\n",
        "print(f\"pooled_embeds shape: {pooled_embeds.shape}\")  # Expected: (1, 1280)\n",
        "# Time IDs for 1024x1024\n",
        "time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], device=device, dtype=torch.float16)\n",
        "print(f\"time_ids shape: {time_ids.shape}\")\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 128, 128),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    print(f\"Step {i}, latent_model_input shape: {latent_model_input.shape}\")\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": pooled_embeds,\n",
        "            \"time_ids\": time_ids\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1, 6, i + 1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")"
      ],
      "metadata": {
        "id": "jf9jaaoH091e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shnikbs509zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    # variant=\"fp16\"  # Try without variant to rule out issues\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "print(\"U-Net config:\", pipe.unet.config)  # Debug U-Net configuration\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "prompt_embeds, _, _, _ = pipe.encode_prompt(prompt)\n",
        "text_inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "pooled_embeds = pipe.text_encoder(\n",
        "    text_inputs,\n",
        "    output_hidden_states=True\n",
        ").pooler_output.to(torch.float16)\n",
        "\n",
        "# Debug shapes\n",
        "print(f\"prompt_embeds[0] shape: {prompt_embeds[0].shape}\")\n",
        "print(f\"pooled_embeds shape: {pooled_embeds.shape}\")\n",
        "\n",
        "# Time IDs for 1024x1024\n",
        "time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], device=device, dtype=torch.float16)\n",
        "print(f\"time_ids shape: {time_ids.shape}\")\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 128, 128),  # 1024x1024\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "# Add hook to debug U-Net layers\n",
        "def hook_fn(module, input, output):\n",
        "    print(f\"Module: {module.__class__.__name__}, Input shape: {input[0].shape}\")\n",
        "\n",
        "for name, module in pipe.unet.named_modules():\n",
        "    if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv2d):\n",
        "        module.register_forward_hook(hook_fn)\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    print(f\"Step {i}, latent_model_input shape: {latent_model_input.shape}\")\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds[0],\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": pooled_embeds,\n",
        "            \"time_ids\": time_ids\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1, 6, i + 1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")"
      ],
      "metadata": {
        "id": "laP1OaF50Fuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Token-level embeddings (cross-attention)\n",
        "prompt_embeds, _, _, _ = pipe.encode_prompt(prompt)  # SDXL returns multiple outputs\n",
        "\n",
        "# Pooled embeddings (for SDXL UNet 'text_time' conditioning)\n",
        "text_inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "pooled_embeds = pipe.text_encoder(\n",
        "    text_inputs,\n",
        "    output_hidden_states=True\n",
        ").pooler_output.to(torch.float16)\n",
        "\n",
        "# Time IDs (required by SDXL UNet)\n",
        "time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], device=device, dtype=torch.float16)  # Adjusted for 1024x1024\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 128, 128),  # Corrected for 1024x1024\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    # Debug: Print shape to verify\n",
        "    print(f\"Step {i}, latent_model_input shape: {latent_model_input.shape}\")\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds[0],  # Use first prompt embed\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": pooled_embeds,\n",
        "            \"time_ids\": time_ids\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")"
      ],
      "metadata": {
        "id": "jo9ExZQfzpvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bWEkIpbuzptR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "#!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn imageio -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "# --- 1. Device and RNG ---\n",
        "device = \"cuda\"  # assume GPU is always available\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# --- 2. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 3. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Token-level embeddings (cross-attention)\n",
        "prompt_embeds = pipe.encode_prompt(prompt)\n",
        "\n",
        "# Pooled embeddings (for SDXL UNet 'text_time' conditioning)\n",
        "text_inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "pooled_embeds = pipe.text_encoder(\n",
        "    text_inputs,\n",
        "    output_hidden_states=True\n",
        ").pooler_output.to(torch.float16)\n",
        "\n",
        "# Time IDs (required by SDXL UNet)\n",
        "time_ids = torch.zeros((1, 6), device=device, dtype=torch.float16)\n",
        "\n",
        "# --- 4. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": pooled_embeds,\n",
        "            \"time_ids\": time_ids\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 5. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 6. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 7. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 8. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 9. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 10. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")\n"
      ],
      "metadata": {
        "id": "EgsnJFBew0-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn imageio -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "\n",
        "# --- 1. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 2. Encode prompt ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Per-token embeddings (pipeline handles tokenization internally)\n",
        "prompt_embeds = pipe.encode_prompt(prompt)\n",
        "\n",
        "# Pooled embeddings for SDXL UNet 'text_time' conditioning\n",
        "text_inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "pooled_embeds = pipe.text_encoder(\n",
        "    text_inputs,\n",
        "    output_hidden_states=True\n",
        ").pooler_output.to(torch.float16)\n",
        "\n",
        "# --- 3. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\"text_embeds\": pooled_embeds}\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 4. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 5. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 6. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 7. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 8. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 9. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")\n"
      ],
      "metadata": {
        "id": "J0lZLsyHv3bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn imageio -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# RNG\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "\n",
        "# --- 1. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 2. Encode prompt via pipeline ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# SDXL pipeline handles tokenization, text encoder projection, and pooled embeddings internally\n",
        "prompt_embeds = pipe.encode_prompt(prompt)\n",
        "\n",
        "# --- 3. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 4. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 5. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 6. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 7. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 8. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 9. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")\n"
      ],
      "metadata": {
        "id": "fgH6CVIzvmQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn imageio\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import imageio\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Device-aware RNG\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "\n",
        "# --- 1. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 2. Prompt embeddings ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Tokenize\n",
        "text_inputs = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# Text encoder outputs\n",
        "outputs = pipe.text_encoder(\n",
        "    text_inputs.input_ids,\n",
        "    attention_mask=text_inputs.attention_mask,\n",
        "    output_hidden_states=True,\n",
        ")\n",
        "\n",
        "prompt_embeds = outputs.last_hidden_state.to(torch.float16)       # (batch, seq_len, hidden_size)\n",
        "pooled_embeds = outputs.pooler_output.to(torch.float16)           # (batch, hidden_size)\n",
        "\n",
        "# Conditioning kwargs\n",
        "add_text_embeds = pooled_embeds\n",
        "add_time_ids = torch.zeros((1, 6), device=device, dtype=torch.float16)\n",
        "\n",
        "# --- 3. Generate anchor latent ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": add_text_embeds,\n",
        "            \"time_ids\": add_time_ids,\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 4. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 5. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode((z_probes / vae.config.scaling_factor).half()).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 6. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 7. Animate along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=z.device, dtype=torch.float16)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode((z / vae.config.scaling_factor).half()).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append((img[0].permute(1,2,0).cpu().numpy() * 255).astype(np.uint8))\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 8. Display first frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"BIG singular mode\")\n",
        "show_frames(frames_small, \"SMALL singular mode\")\n",
        "\n",
        "# --- 9. Export GIFs ---\n",
        "imageio.mimsave(\"animation_big.gif\", frames_big, duration=0.2)\n",
        "imageio.mimsave(\"animation_small.gif\", frames_small, duration=0.2)\n",
        "\n",
        "print(\"GIFs saved: animation_big.gif, animation_small.gif\")\n"
      ],
      "metadata": {
        "id": "7sK7UN1lvPwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Device-aware RNG\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "\n",
        "# --- 1. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 2. Prompt embeddings ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# Tokenize\n",
        "text_inputs = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "# Text encoder outputs\n",
        "outputs = pipe.text_encoder(\n",
        "    text_inputs.input_ids,\n",
        "    attention_mask=text_inputs.attention_mask,\n",
        "    output_hidden_states=True,\n",
        ")\n",
        "\n",
        "prompt_embeds = outputs.last_hidden_state       # (batch, seq_len, hidden_size)\n",
        "pooled_embeds = outputs.pooler_output          # (batch, hidden_size)\n",
        "\n",
        "# Conditioning kwargs for UNet\n",
        "add_text_embeds = pooled_embeds\n",
        "add_time_ids = torch.zeros((1, 6), device=device, dtype=prompt_embeds.dtype)\n",
        "\n",
        "# --- 3. Generate anchor latent from prompt ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": add_text_embeds,\n",
        "            \"time_ids\": add_time_ids,\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 4. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=z.device)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 5. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode(z_probes / vae.config.scaling_factor).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 6. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 7. Animate by stepping along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    # heuristic: map flat image-space singular vector back into latent-shape\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=z.device)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode(z / vae.config.scaling_factor).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append(img[0].permute(1,2,0).cpu().numpy())\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 8. Display frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"Animation along BIG singular mode\")\n",
        "show_frames(frames_small, \"Animation along SMALL singular mode\")\n"
      ],
      "metadata": {
        "id": "ULJFM-sMuzJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 0. Setup ---\n",
        "#!pip install diffusers accelerate transformers safetensors torch torchvision scikit-learn -q\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Device-aware RNG\n",
        "generator = torch.Generator(device=device).manual_seed(1234)\n",
        "\n",
        "# --- 1. Load SDXL pipeline ---\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "    variant=\"fp16\"\n",
        ").to(device, dtype=torch.float16)\n",
        "\n",
        "pipe.enable_vae_tiling()\n",
        "vae = pipe.vae\n",
        "\n",
        "# --- 2. Prompt embeddings ---\n",
        "prompt = \"a surreal city of glass towers at sunset\"\n",
        "\n",
        "# SDXL encode_prompt returns a single tensor\n",
        "prompt_embeds = pipe.encode_prompt(prompt)\n",
        "\n",
        "# pooled embeddings come from the *text encoder’s pooled output*\n",
        "# pipe.encode_prompt does not expose it directly\n",
        "# instead, we use the tokenizer + text_encoder\n",
        "text_inputs = pipe.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipe.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(device)\n",
        "\n",
        "prompt_embeds = pipe.text_encoder(\n",
        "    text_inputs.input_ids,\n",
        "    attention_mask=text_inputs.attention_mask,\n",
        ")[0]  # last hidden states\n",
        "\n",
        "pooled_embeds = pipe.text_encoder(\n",
        "    text_inputs.input_ids,\n",
        "    attention_mask=text_inputs.attention_mask,\n",
        "    output_hidden_states=True,\n",
        ")[1][-1][:, 0]  # pooled CLS-like embedding\n",
        "\n",
        "# --- 3. Conditioning kwargs ---\n",
        "add_text_embeds = pooled_embeds\n",
        "add_time_ids = torch.zeros((1, 6), device=device, dtype=prompt_embeds.dtype)\n",
        "\n",
        "# --- 3b. Generate anchor latent from prompt ---\n",
        "num_inference_steps = 30\n",
        "latents = torch.randn(\n",
        "    (1, pipe.unet.config.in_channels, 64, 64),\n",
        "    device=device,\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "pipe.scheduler.set_timesteps(num_inference_steps, device=device)\n",
        "z_anchor = None\n",
        "\n",
        "for i, t in enumerate(pipe.scheduler.timesteps):\n",
        "    latent_model_input = pipe.scheduler.scale_model_input(latents, t)\n",
        "    noise_pred = pipe.unet(\n",
        "        latent_model_input,\n",
        "        t,\n",
        "        encoder_hidden_states=prompt_embeds,\n",
        "        added_cond_kwargs={\n",
        "            \"text_embeds\": add_text_embeds,\n",
        "            \"time_ids\": add_time_ids,\n",
        "        }\n",
        "    ).sample\n",
        "    step_out = pipe.scheduler.step(noise_pred, t, latents)\n",
        "    latents = step_out.prev_sample if hasattr(step_out, \"prev_sample\") else step_out\n",
        "\n",
        "    if i == 20:  # mid-step anchor\n",
        "        z_anchor = latents.clone().detach()\n",
        "        break\n",
        "\n",
        "if z_anchor is None:\n",
        "    z_anchor = latents.clone().detach()\n",
        "\n",
        "# --- 4. Proximal probes ---\n",
        "def make_probes(z, n_probes=32, eps=0.05):\n",
        "    probes = []\n",
        "    for _ in range(n_probes):\n",
        "        noise = torch.randn_like(z, device=z.device)\n",
        "        noise = noise / (noise.norm() + 1e-12) * (eps * (z.norm() + 1e-12))\n",
        "        probes.append(z + noise)\n",
        "    return torch.cat(probes, dim=0)\n",
        "\n",
        "z_probes = make_probes(z_anchor, n_probes=32, eps=0.05)\n",
        "\n",
        "# --- 5. Decode probes ---\n",
        "with torch.no_grad():\n",
        "    decoded = vae.decode(z_probes / vae.config.scaling_factor).sample\n",
        "\n",
        "imgs = (decoded.clamp(-1, 1) + 1) / 2\n",
        "X = imgs.cpu().flatten(start_dim=1).float().numpy()\n",
        "\n",
        "# --- 6. SVD resonance analysis ---\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "svd.fit(X)\n",
        "singular_vectors = svd.components_\n",
        "print(\"Explained variance ratios:\", svd.explained_variance_ratio_)\n",
        "\n",
        "# --- 7. Animate by stepping along singular directions ---\n",
        "def traverse_singular(z_start, direction_flat, n_frames=12, step_size=0.05):\n",
        "    # heuristic: map flat image-space singular vector back into latent-shape\n",
        "    z = z_start.clone().detach()\n",
        "    latent_dir = torch.randn_like(z, device=z.device)\n",
        "    latent_dir = latent_dir / (latent_dir.norm() + 1e-12) * np.linalg.norm(direction_flat)\n",
        "    frames = []\n",
        "    for i in range(n_frames):\n",
        "        z = z + step_size * latent_dir\n",
        "        with torch.no_grad():\n",
        "            out = vae.decode(z / vae.config.scaling_factor).sample\n",
        "        img = (out.clamp(-1, 1) + 1) / 2\n",
        "        frames.append(img[0].permute(1,2,0).cpu().numpy())\n",
        "    return frames\n",
        "\n",
        "big_mode = singular_vectors[0]\n",
        "small_mode = singular_vectors[-1]\n",
        "\n",
        "frames_big = traverse_singular(z_anchor, big_mode, n_frames=12, step_size=0.05)\n",
        "frames_small = traverse_singular(z_anchor, small_mode, n_frames=12, step_size=0.05)\n",
        "\n",
        "# --- 8. Display frames ---\n",
        "def show_frames(frames, title):\n",
        "    plt.figure(figsize=(12,4))\n",
        "    for i, f in enumerate(frames[:6]):\n",
        "        plt.subplot(1,6,i+1)\n",
        "        plt.imshow(f)\n",
        "        plt.axis(\"off\")\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "show_frames(frames_big, \"Animation along BIG singular mode\")\n",
        "show_frames(frames_small, \"Animation along SMALL singular mode\")\n"
      ],
      "metadata": {
        "id": "jsZTDenVYcL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}