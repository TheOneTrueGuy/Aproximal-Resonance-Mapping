## ARM Progress Report — 2025-09-30

Timestamp: 2025-09-30

### Summary of Work Completed
- Steering core
  - Added real-time layer hooks to inject control vectors at the chosen transformer layer during generation.
  - Corrected manifold-signature steering to use hidden-sized singular vectors (top/right singular vector) from resonance analysis; added dimension/feature-type validation and clear errors.
  - Added basis selection (top-1, single mode, blended modes) with optional weights.

- Interfaces (Analysis + Chat)
  - Exposed basis selection controls in both UIs.
  - Wired new parameters into generation calls with input validation.
  - Performed build/import smoke checks (no launch) and minimal end-to-end generation smoke test.

- Model interface
  - Architecture validation for GPT-2-like requirements (\`transformer.h\`, \`wte\`, \`wpe\`, \`ln_f\`). Provides actionable errors for unsupported models.

- Evaluation harness (examples/arm_eval_harness.py)
  - Added manifold-signature evaluation sweep with signature-cosine metric.
  - Added topology summary (n_clusters, cluster_sizes, silhouette when valid).
  - Kept existing JSON adherence and style transfer dose–response.

### Smoke Tests
- Import/build: arm_interface and arm_chat constructed successfully.
- Minimal generation (control-vector + manifold-signature): completed without exceptions under tiny configs; empty outputs expected for the extremely short settings used.

### Evaluation Metrics Plan (Decision-Focused)
- Manifold quality
  - Clustering: silhouette, (optionally DBI/CH), connected components.
  - Topology: total/mean/max persistence (H0/H1), persistence entropy.
  - Stability across seeds: signature cosine stability, ARI/NMI for clusters, diagram distances.

- Steering effectiveness
  - Mechanism-proximal: signature cosine lift vs baseline; embedding displacement toward target cluster; target-cluster hit rate.
  - Task-level: JSON adherence, style score/classifier; (optional) safety/toxicity direction.
  - Comparative: dose–response AUC; Strength@Threshold; variance/robustness across runs.

- Efficiency
  - Runtime per step; peak memory; throughput vs probes/steps; sensitivity to eps/layer/n_modes.

### TDD Testing Plan
- Unit tests
  - Resonance: rank-1 vs uniform spectra (entropy/PR bounds), shapes of \`top_singular_vectors\`.
  - Topology: shapes and finite values for local persistence; resonance-graph shapes; robust clustering on small n.
  - Steering basis selection: bounds, blend weight handling/normalization, dimension validation.
  - Model interface: GPT-2 validation passes; unsupported arch raises with clear message.

- Integration tests
  - Manifold build (distilgpt2, n=3): valid shapes; silhouette finite/None; labels in range.
  - Generation: non-empty text for control-vector and manifold-signature at normal settings.
  - Harness outputs: CSV/PNG exist with required columns; topology summary fields present.

- Determinism & Guardrails
  - Fix numpy/torch/random seeds; assert no NaNs/Infs; fail fast on incompatible configurations.
  - Include provenance (config + seeds) in saved results.

### Immediate Next Steps
1. Implement unit tests per plan (resonance, topology, steering basis, model validation).
2. Implement integration tests (manifold+clustering, generation non-empty, harness files).
3. Add provenance (config JSON + seed) into harness CSV metadata.
4. Optional: Add CI shard to run unit tests on commit; keep integration as manual/nightly.

### How to Run (reference)
- Analysis UI: `python launch_interface.py` → http://127.0.0.1:7860
- Chat UI: `python launch_chat.py` → http://127.0.0.1:7861
- Evaluation harness: `python examples/arm_eval_harness.py` (outputs to `arm_output/`)

### Execution Decision (Tests & Evals)
- Tests: Run from command line with `pytest` (canonical, CI-able).
- Evaluations: CLI harness is the source of truth for reproducibility.
- UI Convenience: Add a lightweight "Evaluation" tab in Gradio that:
  - Exposes preset runs and minimal parameters
  - Invokes the existing harness functions (no logic duplication)
  - Surfaces key metrics (silhouette, cluster sizes, signature-cosine AUC, Strength@Threshold)
  - Provides links to CSVs/plots and shows provenance (config + seed)

### Evaluation Progress (JSON & Style)
- Initial runs (distilgpt2, gpt2) with greedy/temperature did not yield valid strict JSON; mechanism-proximal metric (signature cosine) showed clear dose–response.
- Switched to gpt2-medium; baseline JSON adherence remained 0.0 across strengths → indicates task difficulty/model limitation rather than steering regression.
- Harness improvements:
  - Added one-shot JSON example to prompt
  - Tried beam search (num_beams=5, early_stopping, no_repeat_ngram_size=2)
  - Added lenient JSON extraction/normalization in scorer
  - Still no strict JSON hits → suggests need for constrained decoding to fairly evaluate formatting tasks

### Plan Adjustments
- Keep JSON task but add optional constrained decoding (applied equally to baseline and steered) for fair comparison.
- Use style and mechanism metrics (signature cosine, embedding displacement) as primary indicators of steering efficacy while formatting remains constrained.


